{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofXQtAKpqlfv"
   },
   "source": [
    "# Basic Regression Analysis - Example 2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYYvcAdGqlfx"
   },
   "source": [
    "Recall OLS assumptions:\n",
    "- Regression residuals normally distributed.\n",
    "- A linear relationship is assumed between the dependent variable and the independent variables.\n",
    "- The residuals are homoscedastic and approximately rectangular-shaped.\n",
    "- Absence of multicollinearity is expected in the model, meaning that independent variables are not too highly correlated.\n",
    "- No Autocorrelation of the residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "770u2FKMqlfz"
   },
   "source": [
    "***\n",
    "## Import our Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIfbejwXqlfz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import diagnostic as diag\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGvAUNNrqlf1"
   },
   "source": [
    "## Load the Data into Pandas\n",
    "Link: https://data.worldbank.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3j8r1vKqlf1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the data and replace the '..' with nan\n",
    "econ_df = pd.read_excel('https://github.com/areed1192/sigma_coding_youtube/raw/master/python/python-data-science/machine-learning/multi-linear-regression/korea_data.xlsx')\n",
    "econ_df = econ_df.replace('..','nan')\n",
    "\n",
    "# set the index to the year column\n",
    "econ_df = econ_df.set_index('Year')\n",
    "\n",
    "# set the data type and select rows up to 2016\n",
    "econ_df = econ_df.astype(float)\n",
    "econ_df = econ_df.loc['1969':'2016']\n",
    "\n",
    "column_names = {'Unemployment, total (% of total labor force) (national estimate)':'unemployment',\n",
    "                'GDP growth (annual %)': 'gdp_growth',\n",
    "                'Gross capital formation (% of GDP)':'gross_capital_formation',\n",
    "                'Population growth (annual %)':'pop_growth', \n",
    "                'Birth rate, crude (per 1,000 people)':'birth_rate',\n",
    "                'Broad money growth (annual %)':'broad_money_growth',                \n",
    "                'Final consumption expenditure (% of GDP)':'final_consum_gdp',\n",
    "                'Final consumption expenditure (annual % growth)':'final_consum_growth',\n",
    "                'General government final consumption expenditure (annual % growth)':'gov_final_consum_growth',\n",
    "                'Gross capital formation (annual % growth)':'gross_cap_form_growth',\n",
    "                'Households and NPISHs Final consumption expenditure (annual % growth)':'hh_consum_growth'}\n",
    "\n",
    "# rename columns\n",
    "econ_df = econ_df.rename(columns = column_names)\n",
    "\n",
    "# check for nulls\n",
    "display('-'*100)\n",
    "display(econ_df.isnull().any())\n",
    "\n",
    "# display the first five rows\n",
    "display('-'*100)\n",
    "display(econ_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4cwQPCOqlf3"
   },
   "source": [
    "## Check for Perfect Multicollinearity\n",
    "\n",
    "\n",
    "**What is multicollinearity?**\n",
    "One explanatory variable is highly correlated with another explanatory variable.\n",
    "\n",
    "**What is the problem with multicollinearity?**\n",
    "Coefficient estimate become unreliable, standard errors inflate, higher probability of wrong conclusions that variable is not statistically significant.\n",
    "\n",
    "**How to deal with multicollinearity?** Check correlations beforehand, or investigate variance inflation factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZixqKNNIqlf4"
   },
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = econ_df.corr()\n",
    "\n",
    "# display the correlation matrix\n",
    "display(corr)\n",
    "\n",
    "# plot the correlation heatmap\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anIx51qgqlf5"
   },
   "source": [
    "**More systematic approach:** Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHy1hLXmqlf6"
   },
   "outputs": [],
   "source": [
    "# define two data frames one before the drop and one after the drop\n",
    "econ_df_before = econ_df\n",
    "econ_df_after = econ_df.drop(['gdp_growth','birth_rate', 'final_consum_growth','gross_capital_formation'], axis = 1)\n",
    "\n",
    "# the VFI does expect a constant term in the data, so we need to add one using the add_constant method\n",
    "X1 = sm.tools.add_constant(econ_df_before)\n",
    "X2 = sm.tools.add_constant(econ_df_after)\n",
    "\n",
    "# create the series for both\n",
    "series_before = pd.Series([variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])], index=X1.columns)\n",
    "series_after = pd.Series([variance_inflation_factor(X2.values, i) for i in range(X2.shape[1])], index=X2.columns)\n",
    "\n",
    "# display the series\n",
    "print('DATA BEFORE')\n",
    "print('-'*100)\n",
    "display(series_before)\n",
    "\n",
    "print('DATA AFTER')\n",
    "print('-'*100)\n",
    "display(series_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Slb1O2Gdqlf7"
   },
   "outputs": [],
   "source": [
    "# define the plot\n",
    "pd.plotting.scatter_matrix(econ_df_after, alpha = 1, figsize = (30, 20))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPOkW9t7qlf7"
   },
   "source": [
    "***\n",
    "## Describe the Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rK9WSLGNqlf8"
   },
   "outputs": [],
   "source": [
    "# get the summary\n",
    "desc_df = econ_df.describe()\n",
    "\n",
    "# add the standard deviation metric\n",
    "desc_df.loc['-3_std'] = desc_df.loc['mean'] - (desc_df.loc['std'] * 3)\n",
    "desc_df.loc['+3_std'] = desc_df.loc['mean'] + (desc_df.loc['std'] * 3)\n",
    "\n",
    "# display it\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15g8yzPdqlf8"
   },
   "source": [
    "\n",
    "### Filtering the Dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNzuzxJNqlf9"
   },
   "outputs": [],
   "source": [
    "# filter the data frame to remove the values exceeding 3 standard deviations\n",
    "econ_remove_df = econ_df[(np.abs(stats.zscore(econ_df)) < 3).all(axis=1)]\n",
    "\n",
    "# what rows were removed\n",
    "econ_df.index.difference(econ_remove_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaYasMMGqlf9"
   },
   "source": [
    "***\n",
    "## Build the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLWjkHkxqlf9"
   },
   "outputs": [],
   "source": [
    "# define our input variable (X) & output variable\n",
    "econ_df_after = econ_df.drop(['birth_rate', 'final_consum_growth','gross_capital_formation'], axis = 1)\n",
    "\n",
    "X = econ_df_after.drop('gdp_growth', axis = 1)\n",
    "Y = econ_df_after[['gdp_growth']]\n",
    "\n",
    "# Split X and y into X_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)\n",
    "\n",
    "# create a Linear Regression model object\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# pass through the X_train & y_train data set\n",
    "regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1otMxkOEqlf-"
   },
   "source": [
    "Exploring the Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mmw5IvFqlf-"
   },
   "outputs": [],
   "source": [
    "# let's grab the coefficient of our model and the intercept\n",
    "intercept = regression_model.intercept_[0]\n",
    "coefficent = regression_model.coef_[0][0]\n",
    "\n",
    "print(\"The intercept for our model is {:.4}\".format(intercept))\n",
    "print('-'*100)\n",
    "\n",
    "# loop through the dictionary and print the data\n",
    "for coef in zip(X.columns, regression_model.coef_[0]):\n",
    "    print(\"The Coefficient for {} is {:.2}\".format(coef[0],coef[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diDY3VjRqlf-"
   },
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUkVPEWkqlf_"
   },
   "outputs": [],
   "source": [
    "# Get multiple predictions\n",
    "y_predict = regression_model.predict(X_test)\n",
    "\n",
    "# Show the first 5 predictions\n",
    "y_predict[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaapo7glqlf_"
   },
   "source": [
    "***\n",
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38a2iwbbqlf_"
   },
   "source": [
    "Using the `Statsmodel`:\n",
    "To make diagnosing the model easier, we will, from this point forward, be using the `statsmodel` module. This module has built-in functions that will make calculating metrics quick. However, we will need \"rebuild\" our model using the `statsmodel` module. We do this by creating a constant variable, call the `OLS()` method and then the `fit()` method. We now have a new model, and the first thing we need to do is to make sure that the assumptions of our model hold. This means checking the following:\n",
    "\n",
    "- Regression residuals must be normally distributed.\n",
    "- The residuals are homoscedastic\n",
    "- Absence of multicollinearity (we did this above).\n",
    "- No Autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVfGXzl1qlgA"
   },
   "outputs": [],
   "source": [
    "# define our intput\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "# create a OLS model\n",
    "model = sm.OLS(Y, X2)\n",
    "\n",
    "# fit the data\n",
    "est = model.fit()\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwJvSewlqlgA"
   },
   "source": [
    "### Checking for Heteroscedasticity\n",
    "**What is Heteroscedasticity?**  Simple definition: standard errors of a variable, monitored over a specific amount of time, are non-constant. \n",
    "Example: household consumption based on income; variability of expenditures changes depending on how much income you have. Households with more income spend money on a broader set of items compared to lower income households that would only be able to focus on the main staples --> results in standard errors that change over income levels.\n",
    "\n",
    "**What is the problem with heteroscedasticity**\n",
    "***\n",
    "1. Coefficient estimates to be less precise.\n",
    "2. Heteroscedasticity tends to produce p-values that are smaller than they should be.\n",
    "\n",
    "\n",
    "**How to test for heteroscedasticity?**\n",
    "Breusch-Pagan and the White test for heteroscedasticity. The Breusch-Pagan is a more general test for heteroscedasticity while the White test is a unique case.\n",
    "\n",
    "- The null hypothesis for both the White’s test and the Breusch-Pagan test is that the variances for the errors are equal:\n",
    "    - **H0 = σ2i = σ2**\n",
    "- The alternate hypothesis (the one you’re testing), is that the variances are not equal:\n",
    "    - **H1 = σ2i ≠ σ2**\n",
    "\n",
    "Our goal is to fail to reject the null hypothesis, have a high p-value because that means we have no heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Lr36ot8qlgI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the White's test\n",
    "_, pval, __, f_pval = diag.het_white(est.resid, est.model.exog)\n",
    "print(pval, f_pval)\n",
    "print('-'*100)\n",
    "\n",
    "# print the results of the test\n",
    "if pval > 0.05:\n",
    "    print(\"For the White's Test\")\n",
    "    print(\"The p-value was {:.4}\".format(pval))\n",
    "    print(\"We fail to reject the null hypthoesis, so there is no heteroscedasticity. \\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"For the White's Test\")\n",
    "    print(\"The p-value was {:.4}\".format(pval))\n",
    "    print(\"We reject the null hypthoesis, so there is heteroscedasticity. \\n\")\n",
    "\n",
    "# Run the Breusch-Pagan test\n",
    "_, pval, __, f_pval = diag.het_breuschpagan(est.resid, est.model.exog)\n",
    "print(pval, f_pval)\n",
    "print('-'*100)\n",
    "\n",
    "# print the results of the test\n",
    "if pval > 0.05:\n",
    "    print(\"For the Breusch-Pagan's Test\")\n",
    "    print(\"The p-value was {:.4}\".format(pval))\n",
    "    print(\"We fail to reject the null hypthoesis, so there is no heteroscedasticity.\")\n",
    "\n",
    "else:\n",
    "    print(\"For the Breusch-Pagan's Test\")\n",
    "    print(\"The p-value was {:.4}\".format(pval))\n",
    "    print(\"We reject the null hypthoesis, so there is heteroscedasticity.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIZPq1xVqlgI"
   },
   "source": [
    "How to deal with heteroscedasticity in case?\n",
    "Use robust standard errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAN67w-VqlgJ"
   },
   "outputs": [],
   "source": [
    "# create a OLS model\n",
    "model = sm.OLS(Y, X2)\n",
    "\n",
    "# fit the data\n",
    "robust_ols = model.fit(cov_type='HC1')\n",
    "print(robust_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY2Ekq64qlgJ"
   },
   "source": [
    "### Checking for Autocorrelation\n",
    "**What is autocorrelation?** Autocorrelation is a characteristic of data in which the correlation between the values of the same variables is based on related objects. \n",
    "***\n",
    "**What is the problem with autocorrelation?** Computed standard errors, and consequently p-values, are misleading. Autocorrelation in the residuals of a model is also a sign that the model may be unsound. A workaround is we can compute more robust standard errors.\n",
    "\n",
    "***\n",
    "**How to test for autocorrelation?** Again, use `statsmodels.stats.diagnostic` module to conduct the Ljung-Box test for no autocorrelation of residuals. Here:\n",
    "\n",
    "- **H0: The data are random.**\n",
    "- **Ha: The data are not random.**\n",
    "\n",
    "That means we want to fail to reject the null hypothesis, have a large p-value because then it means we have no autocorrelation. To use the Ljung-Box test, we will call the `acorr_ljungbox` function, pass through the `est.resid` and then define the lags. \n",
    "\n",
    "The lags can either be calculated by the function itself, or we can calculate them. If the function handles it the max lag will be `min((num_obs // 2 - 2), 40)`, however, there is a rule of thumb that for non-seasonal time series the lag is ` min(10, (num_obs // 5))`.\n",
    "\n",
    "We also can visually check for autocorrelation by using the `statsmodels.graphics` module to plot a graph of the autocorrelation factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXsqLYkTqlgJ"
   },
   "outputs": [],
   "source": [
    "# test for autocorrelation\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# calculate the lag, optional\n",
    "lag = min(10, (len(X)//5))\n",
    "print('The number of lags will be {}'.format(lag))\n",
    "print('-'*100)\n",
    "\n",
    "# run the Ljung-Box test for no autocorrelation of residuals\n",
    "# test_results = diag.acorr_breusch_godfrey(est, nlags = lag, store = True)\n",
    "test_results = diag.acorr_ljungbox(est.resid, lags = lag)\n",
    "\n",
    "# grab the p-values and the test statistics\n",
    "ibvalue, p_val = test_results\n",
    "\n",
    "# print the results of the test\n",
    "if min(p_val) > 0.05:\n",
    "    print(\"The lowest p-value found was {:.4}\".format(min(p_val)))\n",
    "    print(\"We fail to reject the null hypthoesis, so there is no autocorrelation.\")\n",
    "    print('-'*100)\n",
    "else:\n",
    "    print(\"The lowest p-value found was {:.4}\".format(min(p_val)))\n",
    "    print(\"We reject the null hypthoesis, so there is autocorrelation.\")\n",
    "    print('-'*100)\n",
    "\n",
    "# plot autocorrelation\n",
    "sm.graphics.tsa.plot_acf(est.resid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPFfRIJ4qlgK"
   },
   "source": [
    "### Checking For Normally Distributed Residuals and if the Mean of the Residuals Equals 0\n",
    "\n",
    "We do both visually with QQplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSZE1F_gqlgK"
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "\n",
    "# check for the normality of the residuals\n",
    "sm.qqplot(est.resid, line='s')\n",
    "pylab.show()\n",
    "\n",
    "# also check that the mean of the residuals is approx. 0.\n",
    "mean_residuals = sum(est.resid)/ len(est.resid)\n",
    "print(\"The mean of the residuals is {:.4}\".format(mean_residuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBy6bY-gqlgK"
   },
   "source": [
    "#### Measures of Error\n",
    "\n",
    "We can examine how well our data fit the model, so we will take `y_predictions` and compare them to our `y_actuals` these will be our residuals. From here we can calculate a few metrics to help quantify how well our model fits the data. Here are a few popular metrics:\n",
    "\n",
    "- **Mean Absolute Error (MAE):** Is the mean of the absolute value of the errors. This gives an idea of magnitude but no sense of direction (too high or too low).\n",
    "\n",
    "- **Mean Squared Error (MSE):** Is the mean of the squared errors. MSE is more popular than MAE because MSE \"punishes\" more significant errors.\n",
    "\n",
    "- **Root Mean Squared Error (RMSE):** Is the square root of the mean of the squared errors. RMSE is even more favored because it allows us to interpret the output in y-units.\n",
    "\n",
    "Luckily for us, `sklearn` and `statsmodel` both contain functions that will calculate these metrics for us. The examples below were calculated using the `sklearn` library and the `math` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKG8coPYqlgK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# calculate the mean squared error\n",
    "model_mse = mean_squared_error(y_test, y_predict)\n",
    "\n",
    "# calculate the mean absolute error\n",
    "model_mae = mean_absolute_error(y_test, y_predict)\n",
    "\n",
    "# calulcate the root mean squared error\n",
    "model_rmse =  math.sqrt(model_mse)\n",
    "\n",
    "# display the output\n",
    "print(\"MSE {:.3}\".format(model_mse))\n",
    "print(\"MAE {:.3}\".format(model_mae))\n",
    "print(\"RMSE {:.3}\".format(model_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za83Vpa3qlgL"
   },
   "source": [
    "***\n",
    "### R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwJYVwqcqlgL"
   },
   "outputs": [],
   "source": [
    "model_r2 = r2_score(y_test, y_predict)\n",
    "print(\"R2: {:.2}\".format(model_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41A97SgmqlgL"
   },
   "source": [
    "***\n",
    "### Confidence Intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn6AFwdTqlgL"
   },
   "outputs": [],
   "source": [
    "# make some confidence intervals, 95% by default\n",
    "est.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXhMR6wtqlgM"
   },
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "With hypothesis testing, we are trying to determine the statistical significance of the coefficient estimates. This test is outlined as the following.\n",
    "\n",
    "- **Null Hypothesis:** There is no relationship between the exploratory variables and the explanatory variable.\n",
    "- **Alternative Hypothesis:** There is a relationship between the exploratory variables and the explanatory variable.\n",
    "***\n",
    "- If we **reject the null**, we are saying there is a relationship, and the coefficients do not equal 0.\n",
    "- If we **fail to reject the null**, we are saying there is no relationship, and the coefficients do equal 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THwHVqbtqlgM"
   },
   "outputs": [],
   "source": [
    "# estimate the p-values\n",
    "est.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKicJRfRqlgM"
   },
   "source": [
    "Here it's a little hard to tell, but we have a few insignificant coefficients. The first is the constant itself, so technically this should be dropped. However, we will see that once we remove the irrelevant variables that the intercept becomes significant. **If it still wasn't significant, we could have our intercept start at 0 and assume that the cumulative effect of X on Y begins from the origin (0,0).** Along with the constant, we have `unemployment` and `broad_money_growth` both come out as insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9sQ9EkfqlgM"
   },
   "source": [
    "***\n",
    "### Create a Summary of the Model Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCc4MkgUqlgN"
   },
   "outputs": [],
   "source": [
    "# print out a summary\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNUuM9S2qlgN"
   },
   "source": [
    "***\n",
    "## Remove the Insignificant Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8TmDC2oqlgN"
   },
   "outputs": [],
   "source": [
    "# define our input variable (X) & output variable\n",
    "econ_df_after = econ_df.drop(['birth_rate', 'final_consum_growth','gross_capital_formation','broad_money_growth',\n",
    "                              'unemployment'], axis = 1)\n",
    "\n",
    "X = econ_df_after.drop('gdp_growth', axis = 1)\n",
    "Y = econ_df_after[['gdp_growth']]\n",
    "\n",
    "# Split X and y into X_\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)\n",
    "\n",
    "# create a Linear Regression model object\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# pass through the X_train & y_train data set\n",
    "regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhrP3toNqlgN"
   },
   "outputs": [],
   "source": [
    "# define our intput\n",
    "X2 = sm.add_constant(X)\n",
    "\n",
    "# create a OLS model\n",
    "model = sm.OLS(Y, X2)\n",
    "\n",
    "# fit the data\n",
    "est = model.fit()\n",
    "\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xm3fMvZtqlgN"
   },
   "source": [
    "***\n",
    "## Save the Model for Future Use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm4J8tgJqlgO"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle the model\n",
    "with open('my_mulitlinear_regression.sav','wb') as f:\n",
    "     pickle.dump(regression_model, f)\n",
    "\n",
    "# load it back in\n",
    "with open('my_mulitlinear_regression.sav', 'rb') as pickle_file:\n",
    "     regression_model_2 = pickle.load(pickle_file)\n",
    "\n",
    "# make a new prediction\n",
    "regression_model_2.predict([X_test.loc[2002]])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "7_2_Basic_Regression_Analysis_Example_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "313px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
